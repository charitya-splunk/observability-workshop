<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Monitoring Horizontal Pod Autoscaling in Kubernetes :: Splunk Observability Cloud Workshops</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/index.html</link><description>This workshop will equip you with the basic understanding of monitoring Kubernetes using the Splunk OpenTelemetry Collector</description><generator>Hugo</generator><language>en</language><atom:link href="https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/index.xml" rel="self" type="application/rss+xml"/><item><title>Deploying the OpenTelemetry Collector in Kubernetes using a NameSpace</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/1-deploy-otel/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/1-deploy-otel/index.html</guid><description>1. Kubernetes Navigator 2.0 UI We will be starting this workshop using the new Kubernetes Navigator so please check that you are already using the new Navigator.
When you select Infrastructure from the main menu on the left, followed by selecting Kubernetes, you should see two service panes (K8s nodes and K8s workloads) for Kubernetes, similar to the ones below:</description></item><item><title>Tour of the Kubernetes Navigator</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/2-check-new-navigator-short/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/2-check-new-navigator-short/index.html</guid><description>1. Cluster vs Workload View The Kubernetes Navigator offers you two separate use cases to view your Kubernetes data.
The K8s workloads are focusing on providing information in regards to workloads a.k.a. your deployments. The K8s nodes are focusing on providing insight into the performance of clusters, nodes, pods and containers. You will initially select either view depending on your need (you can switch between the view on the fly if required). The most common one we will use in this workshop is the workload view and we will focus on that specifically.</description></item><item><title>Deploying PHP/Apache</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/3-deploy-apache/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/3-deploy-apache/index.html</guid><description>1. Namespaces in Kubernetes Most of our customers will make use of some kind of private or public cloud service to run Kubernetes. They often choose to have only a few large Kubernetes clusters as it is easier to manage centrally.
Namespaces are a way to organize these large Kubernetes clusters into virtual sub-clusters. This can be helpful when different teams or projects share a Kubernetes cluster as this will give them the easy ability to just see and work with their resources.</description></item><item><title>Fix PHP/Apache Issue</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/4-fix-apache/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/4-fix-apache/index.html</guid><description>1. Kubernetes Resources Especially in Production Kubernetes Clusters, CPU and Memory are considered precious resources. Cluster Operators will normally require you to specify the amount of CPU and Memory your Pod or Service will require in the deployment, so they can have the Cluster automatically manage on which Node(s) your solution will be placed.
You do this by placing a Resource section in the deployment of your application/Pod
Example:
resources: limits: # Maximum amount of CPU &amp; memory for peek use cpu: "8" # Maximum of 8 cores of CPU allowed at for peek use memory: "8Mi" # Maximum allowed 8Mb of memory requests: # Request are the expected amount of CPU &amp; memory for normal use cpu: "6" # Requesting 4 cores of a CPU memory: "4Mi" # Requesting 4Mb of memory More information can be found here: Resource Management for Pods and Containers</description></item><item><title>Deploy Load Generator</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/5-deploy-loadgen/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/5-deploy-loadgen/index.html</guid><description>Now letâ€™s apply some load against the php-apache pod. To do this, you will need to start a different Pod to act as a client. The container within the client Pod runs in an infinite loop, sending HTTP GETs to the php-apache service.
1. Review loadgen YAML Inspect the YAML file ~/workshop/k3s/loadgen.yaml and validate the contents using the following command:
cat ~/workshop/k3s/loadgen.yaml This file contains the configuration for the load generator and will create a new ReplicaSet with two replicas of the load generator image.</description></item><item><title>Setup Horizontal Pod Autoscaling (HPA)</title><link>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/6-setup-hpa/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/ninja-workshops/2-hpa/6-setup-hpa/index.html</guid><description>In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), to automatically scale the workload to match demand.
Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.
If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.</description></item></channel></rss>