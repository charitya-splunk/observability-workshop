<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Getting Data In (GDI) with OTel and UF :: Splunk Observability Cloud Workshops</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/index.html</link><description>Learn how to get data into Splunk Observability Cloud with OpenTelemetry and the Splunk Universal Forwarder.</description><generator>Hugo</generator><language>en</language><atom:link href="https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting Started with O11y GDI - Real Time Enrichment Workshop</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/1-getting-started/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/1-getting-started/index.html</guid><description>Please note to begin the following lab, you must have completed the prework:
Obtain a Splunk Observability Cloud access key Understand cli commands Follow these steps if using O11y Workshop EC2 instances
1. Verify yelp data files are present ll /var/appdata/yelp* 2. Export the following variables export ACCESS_TOKEN=&lt;your-access-token> export REALM=&lt;your-o11y-cloud-realm> export clusterName=&lt;your-k8s-cluster> 3. Clone the following repo cd /home/splunk git clone https://github.com/leungsteve/realtime_enrichment.git cd realtime_enrichment/workshop python3 -m venv rtapp-workshop source rtapp-workshop/bin/activate</description></item><item><title>Deploy Complex Environments and Capture Metrics</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/2-deploy/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/2-deploy/index.html</guid><description>Objective: Learn how to efficiently deploy complex infrastructure components such as Kafka and MongoDB to demonstrate metrics collection with Splunk O11y IM integrations
Duration: 15 Minutes
Scenario A prospect uses Kafka and MongoDB in their environment. Since there are integrations for these services, you’d like to demonstrate this to the prospect. What is a quick and efficient way to set up a live environment with these services and have metrics collected?</description></item><item><title>Code to Kubernetes - Python</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/3-code-to-kubernetes/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/3-code-to-kubernetes/index.html</guid><description>Code to Kubernetes - Python Objective: Understand activities to instrument a python application and run it on Kubernetes.
Verify the code Containerize the app Deploy the container in Kubernetes Note: these steps do not involve Splunk
Duration: 15 Minutes
1. Verify the code - Review service Navigate to the review directory
cd /home/splunk/realtime_enrichment/flask_apps/review/ Inspect review.py (realtime_enrichment/flask_apps/review)
cat review.py from flask import Flask, jsonify import random import subprocess review = Flask(__name__) num_reviews = 8635403 num_reviews = 100000 reviews_file = '/var/appdata/yelp_academic_dataset_review.json' @review.route('/') def hello_world(): return jsonify(message='Hello, you want to hit /get_review. We have ' + str(num_reviews) + ' reviews!') @review.route('/get_review') def get_review(): random_review_int = str(random.randint(1,num_reviews)) line_num = random_review_int + 'q;d' command = ["sed", line_num, reviews_file] # sed "7997242q;d" &lt;file> random_review = subprocess.run(command, stdout=subprocess.PIPE, text=True) return random_review.stdout if __name__ == "__main__": review.run(host ='0.0.0.0', port = 5000, debug = True) Inspect requirements.txt</description></item><item><title>Instrument REVIEWS for Tracing</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/4-instrument/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/4-instrument/index.html</guid><description>1. Use Data Setup to instrument a Python application Within the O11y Cloud UI:
Data Management -> Add Integration -> Monitor Applications -> Python (traces) -> Add Integration
Provide the following to the Configure Integration Wizard:
Service: review
Django: no
collector endpoint: http://localhost:4317
Environment: rtapp-workshop-[YOURNAME]
Kubernetes: yes
Legacy Agent: no
We are instructed to:
Install the instrumentation packages for your Python environment. pip install splunk-opentelemetry[all] splunk-py-trace-bootstrap Configure the Downward API to expose environment variables to Kubernetes resources.</description></item><item><title>Monitor System Logs with Splunk Universal Forwarder</title><link>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/5-forwarder/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.70/en/unsupported-field-workshops/8-gdi/5-forwarder/index.html</guid><description>Objective: Learn how to monitor Linux system logs with the Universal Forwarder sending logs to Splunk Enterprise
Duration: 10 Minutes
Scenario You’ve been tasked with monitoring the OS logs of the host running your Kubernetes cluster. We are going to utilize a script that will autodeploy the Splunk Universal Forwarder. You will then configure the Universal Forwarder to send logs to the Splunk Enterprise instance assigned to you.
1. Ensure You’re in the Correct Directory we will need to be in /home/splunk/session-2 cd /home/splunk/session-2 2. Review the Universal Forwarder Install Script Let’s take a look at the script that will install the Universal Forwarder and Linux TA automatically for you. This script is primarily used for remote instances. Note we are not using a deployment server in this lab, however it is recommended in production we do that. What user are we installing Splunk as? #!/bin/sh # This EXAMPLE script shows how to deploy the Splunk universal forwarder # to many remote hosts via ssh and common Unix commands. # For "real" use, this script needs ERROR DETECTION AND LOGGING!! # --Variables that you must set ----- # Set username using by splunkd to run. SPLUNK_RUN_USER="ubuntu" # Populate this file with a list of hosts that this script should install to, # with one host per line. This must be specified in the form that should # be used for the ssh login, ie. username@host # # Example file contents: # splunkuser@10.20.13.4 # splunkker@10.20.13.5 HOSTS_FILE="myhost.txt" # This should be a WGET command that was *carefully* copied from splunk.com!! # Sign into splunk.com and go to the download page, then look for the wget # link near the top of the page (once you have selected your platform) # copy and paste your wget command between the "" WGET_CMD="wget -O splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz 'https://download.splunk.com/products/universalforwarder/releases/9.0.3/linux/splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz'" # Set the install file name to the name of the file that wget downloads # (the second argument to wget) INSTALL_FILE="splunkforwarder-9.0.3-dd0128b1f8cd-Linux-x86_64.tgz" # After installation, the forwarder will become a deployment client of this # host. Specify the host and management (not web) port of the deployment server # that will be managing these forwarder instances. # Example 1.2.3.4:8089 # DEPLOY_SERVER="x.x.x.x:8089" # After installation, the forwarder can have additional TA's added to the # /app directory please provide the local where TA's will be. TA_INSTALL_DIRECTORY="/home/splunk/session-2" # Set the seed app folder name for deploymentclien.conf # DEPLOY_APP_FOLDER_NAME="seed_all_deploymentclient" # Set the new Splunk admin password PASSWORD="buttercup" REMOTE_SCRIPT_DEPLOY=" cd /opt sudo $WGET_CMD sudo tar xvzf $INSTALL_FILE sudo rm $INSTALL_FILE #sudo useradd $SPLUNK_RUN_USER sudo find $TA_INSTALL_DIRECTORY -name '*.tgz' -exec tar xzvf {} --directory /opt/splunkforwarder/etc/apps \; sudo chown -R $SPLUNK_RUN_USER:$SPLUNK_RUN_USER /opt/splunkforwarder echo \"[user_info] USERNAME = admin PASSWORD = $PASSWORD\" > /opt/splunkforwarder/etc/system/local/user-seed.conf #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwader/etc/apps/ #sudo find /opt/splunkforwarder/etc/apps/ -name '*.tgz' -exec tar xzvf {} \; #sudo -u splunk /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt /opt/splunkforwarder/bin/splunk start --accept-license --answer-yes --auto-ports --no-prompt #sudo /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER /opt/splunkforwarder/bin/splunk enable boot-start -user $SPLUNK_RUN_USER #sudo cp $TA_INSTALL_DIRECTORY/*.tgz /opt/splunkforwarder/etc/apps/ exit " DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null &amp;&amp; pwd )" #=============================================================================================== echo "In 5 seconds, will run the following script on each remote host:" echo echo "====================" echo "$REMOTE_SCRIPT_DEPLOY" echo "====================" echo sleep 5 echo "Reading host logins from $HOSTS_FILE" echo echo "Starting." for DST in `cat "$DIR/$HOSTS_FILE"`; do if [ -z "$DST" ]; then continue; fi echo "---------------------------" echo "Installing to $DST" echo "Initial UF deployment" sudo ssh -t "$DST" "$REMOTE_SCRIPT_DEPLOY" done echo "---------------------------" echo "Done" echo "Please use the following app folder name to override deploymentclient.conf options: $DEPLOY_APP_FOLDER_NAME" 3. Run the install script We will run the install script now. You will see some Warnings at the end. This is totally normal. The script is built for use on remote machines, however for todays lab you will be using localhost.</description></item></channel></rss>