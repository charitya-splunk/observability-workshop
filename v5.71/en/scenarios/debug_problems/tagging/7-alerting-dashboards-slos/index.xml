<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Use Tags for Monitoring :: Splunk Observability Cloud Workshops</title><link>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/index.html</link><description>Earlier, we created a Troubleshooting Metric Set on the credit.score.category tag, which allowed us to use Tag Spotlight with that tag and identify a pattern to explain why some users received a poor experience.
In this section of the workshop, we’ll explore a related concept: Monitoring MetricSets.
What are Monitoring MetricSets? Monitoring MetricSets go beyond troubleshooting and allow us to use tags for alerting, dashboards and SLOs.
Create a Monitoring MetricSet (note: your workshop instructor will do the following for you, but observe the steps)</description><generator>Hugo</generator><language>en</language><atom:link href="https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/index.xml" rel="self" type="application/rss+xml"/><item><title>Use Tags with Dashboards</title><link>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/1-dashboards/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/1-dashboards/index.html</guid><description>Dashboards Navigate to Metric Finder, then type in the name of the tag, which is credit_score_category (remember that the dots in the tag name were replaced by underscores when the Monitoring MetricSet was created). You’ll see that multiple metrics include this tag as a dimension:
By default, Splunk Observability Cloud calculates several metrics using the trace data it receives. See Learn about MetricSets in APM for more details.</description></item><item><title>Use Tags with Alerting</title><link>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/2-alerting/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/2-alerting/index.html</guid><description>Alerts It’s great that we have a dashboard to monitor the response times of the credit check service by credit score, but we don’t want to stare at a dashboard all day.
Let’s create an alert so we can be notified proactively if customers with exceptional credit scores encounter slow requests.
To create this alert, click on the little bell on the top right-hand corner of the chart, then select New detector from chart:</description></item><item><title>Use Tags with Service Level Objectives</title><link>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/3-slos/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://splunk.github.io/observability-workshop/v5.71/en/scenarios/debug_problems/tagging/7-alerting-dashboards-slos/3-slos/index.html</guid><description>We can now use the created Monitoring MetricSet together with Service Level Objectives a similar way we used them with dashboards and detectors/alerts before. For that we want to be clear about some key concepts:
Key Concepts of Service Level Monitoring (skip if you know this)
Concept Definition Examples Service level indicator (SLI) An SLI is a quantitative measurement showing some health of a service, expressed as a metric or combination of metrics. Availability SLI: Proportion of requests that resulted in a successful response
Performance SLI: Proportion of requests that loaded in &lt; 100 ms Service level objective (SLO) An SLO defines a target for an SLI and a compliance period over which that target must be met. An SLO contains 3 elements: an SLI, a target, and a compliance period. Compliance periods can be calendar, such as monthly, or rolling, such as past 30 days. Availability SLI over a calendar period: Our service must respond successfully to 95% of requests in a month
Performance SLI over a rolling period: Our service must respond to 99% of requests in &lt; 100 ms over a 7-day period Service level agreement (SLA) An SLA is a contractual agreement that indicates service levels your users can expect from your organization. If an SLA is not met, there can be financial consequences. A customer service SLA indicates that 90% of support requests received on a normal support day must have a response within 6 hours. Error budget A measurement of how your SLI performs relative to your SLO over a period of time. Error budget measures the difference between actual and desired performance. It determines how unreliable your service might be during this period and serves as a signal when you need to take corrective action. Our service can respond to 1% of requests in >100 ms over a 7 day period. Burn rate A unitless measurement of how quickly a service consumes the error budget during the compliance window of the SLO. Burn rate makes the SLO and error budget actionable, showing service owners when a current incident is serious enough to page an on-call responder. For an SLO with a 30-day compliance window, a constant burn rate of 1 means your error budget is used up in exactly 30 days. Creating a new Service Level Objective There is an easy to follow wizard to create a new Service Level Objective (SLO). In the left navigation just follow the link “Detectors &amp; SLOs”. From there select the third tab “SLOs” and click the blue button to the right that says “Create SLO”.</description></item></channel></rss>